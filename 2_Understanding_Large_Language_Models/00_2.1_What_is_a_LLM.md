# 00_2.1_What_is_a_LLM

"""
Lecture: /2_Understanding_Large_Language_Models
Content: 00_2.1_What_is_a_LLM
"""

### 2.1 什么是大型语言模型（LLM）

#### 1. 引言
大型语言模型（Large Language Models，LLMs）是基于神经网络设计的，旨在理解、生成和回应类人文本的模型。这些模型是深度神经网络，通过海量文本数据进行训练，有时甚至涵盖了互联网上所有公开可用的文本的很大一部分。

#### 2. LLM的大小
“大型”指的是模型的参数规模和训练所用的数据集的庞大。这样的模型通常拥有数十亿甚至数千亿个参数，这些参数是在训练过程中优化的可调权重，用于预测序列中的下一个词。预测下一个词的任务非常简单，但它利用了语言的固有顺序性，从而使模型能够训练对文本的上下文、结构和关系的理解。

#### 3. Transformer架构的成功
LLM的成功可以归功于Transformer架构，这种架构支持许多LLM，并且这些模型的训练数据量庞大，使其能够捕捉到大量的语言细微差别、上下文和模式，这些是手工编码难以实现的。

#### 4. LLM的应用
LLM在广泛的自然语言处理（NLP）任务中显著提升了性能，包括文本翻译、情感分析、问答等。现代的LLM展示了在广泛的NLP任务上的出色能力，而早期的NLP模型通常是为特定任务设计的，在其狭窄的应用领域表现优异，而LLM展示了跨广泛NLP任务的更广泛能力。

#### 5. LLM的训练
LLM使用一种称为Transformer的架构，它允许模型在进行预测时对输入的不同部分给予选择性关注，使其特别擅长处理人类语言的细微差别和复杂性。由于LLM能够生成文本，因此它们通常被称为一种生成性人工智能（Generative AI，简称为GenAI）。AI包括创建能够执行需要人类智能的任务的机器的广泛领域，LLM代表了深度学习技术的一个具体应用，利用它们处理和生成类人文本的能力。

#### 6. LLM的预训练
LLM通常在一个庞大、广泛的数据集上进行初始预训练，以发展广泛的语言理解能力。这个预训练的模型称为基础模型，然后可以通过微调过程进一步训练，使其在特定的任务或领域表现更好。微调的两种主要类型包括指令微调和分类任务微调。

#### 7. 总结
LLM通过对大量文本数据的训练，捕捉到语言的细微差别和复杂性，展示了在广泛的NLP任务上的卓越能力。它们的成功很大程度上依赖于Transformer架构和大规模数据集的使用，使其成为现代NLP的强大工具。