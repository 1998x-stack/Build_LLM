# 00_3.1_Understanding_word_embeddings

"""
Lecture: /3_Working_with_Text_Data
Content: 00_3.1_Understanding_word_embeddings
"""

### 3.1 理解词嵌入

#### 1. 引言
在深度学习和自然语言处理（NLP）领域，词嵌入（Word Embeddings）是将词汇映射到连续向量空间的一种方法，使得文本数据能够被神经网络处理。通过词嵌入，离散的词汇可以转换为数值向量，便于进行数学操作和模型训练。

#### 2. 词嵌入的基本概念
词嵌入的核心是将离散的对象（如单词、图片或文档）映射到连续向量空间中。这种转换的主要目的是将非数值数据转换为神经网络可以处理的格式。最常见的文本嵌入形式是词嵌入，同时也有句子嵌入、段落嵌入和文档嵌入。

#### 3. 词嵌入算法和框架
多个算法和框架已经被开发出来用于生成词嵌入。其中一个早期且最受欢迎的例子是Word2Vec。Word2Vec通过预测给定目标词的上下文来训练神经网络架构生成词嵌入。其主要思想是出现在相似上下文中的词往往具有相似的意义。例如，在可视化时，使用Word2Vec生成的二维词嵌入，表示相似概念的词往往聚集在一起。

#### 4. 维度和可视化
词嵌入的维度可以从一维到数千维不等。较高的维度可以捕捉到更多的细微关系，但也会增加计算复杂性。虽然我们可以使用预训练的模型（如Word2Vec）生成嵌入，但大型语言模型（LLMs）通常会生成自己的嵌入，并在训练过程中更新这些嵌入。通过在LLM训练中优化嵌入，可以使嵌入适应特定的任务和数据。

#### 5. 上下文化输出嵌入
LLMs不仅能够生成词嵌入，还可以生成上下文化的输出嵌入。高维嵌入的可视化是一个挑战，因为我们的感官和常见的图形表示方式通常局限于三维或更少。然而，在处理LLMs时，我们通常使用更高维度的嵌入。GPT-2和GPT-3等模型的嵌入维度基于具体的模型变体和规模有所不同，例如，GPT-3的嵌入维度高达12,288。

#### 6. 嵌入的创建和使用
为了创建LLMs使用的嵌入，首先需要将文本分割成单词或子词，然后将这些词转换为嵌入向量。具体步骤包括：
1. **分词**：将输入文本切分为单词或子词。
2. **词ID转换**：将分词转换为词ID。
3. **嵌入向量生成**：使用嵌入层将词ID转换为嵌入向量。

#### 7. 位置嵌入
为了增强模型对序列中词语顺序的理解，我们还需要为每个词添加位置信息。这可以通过绝对位置嵌入或相对位置嵌入来实现。绝对位置嵌入直接与序列中的特定位置关联，而相对位置嵌入则强调词语之间的相对位置。

#### 8. 总结
理解和实现词嵌入是构建LLM的基础步骤。通过将词汇转换为高维向量，并结合位置嵌入，可以为LLM提供丰富的上下文信息，增强其文本生成和理解能力。这一过程依赖于深度学习技术和大规模数据集，是现代NLP技术的重要组成部分。