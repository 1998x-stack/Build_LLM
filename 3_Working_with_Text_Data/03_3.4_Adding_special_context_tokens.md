# 03_3.4_Adding_special_context_tokens

"""
Lecture: /3_Working_with_Text_Data
Content: 03_3.4_Adding_special_context_tokens
"""

### 2.4 添加特殊上下文标记

#### 背景介绍
在上一节中，我们实现了一个简单的标记器，并将其应用于训练集中的文本段落。在这一节中，我们将修改该标记器，以处理未知单词和添加特殊上下文标记。这些特殊标记可以包括表示未知单词和文档边界的标记。

#### 为什么需要特殊上下文标记
当处理大型语言模型（LLM）时，常常会遇到训练集中未包含的单词，或需要在不同文档或文本段落之间添加分隔符，以帮助模型理解上下文。例如，我们可以添加一个 `<|unk|>` 标记来表示新的未知单词，或添加一个 `<|endoftext|>` 标记来分隔两个不相关的文本源。

##### 图2.9：词汇表中添加特殊标记
如图2.9所示，我们可以修改标记器，使其在遇到不在词汇表中的单词时使用 `<|unk|>` 标记。此外，我们可以在不相关的文本之间添加一个 `<|endoftext|>` 标记。例如，当在多个独立文档或书籍上训练GPT类的LLM时，通常在每个文档或书籍之前插入一个标记，以帮助LLM理解这些文本源虽然被连接在一起进行训练，但实际上是不相关的。

##### 图2.10：处理多个独立文本源时添加 `<|endoftext|>` 标记
当处理多个独立的文本源时，我们在这些文本之间添加 `<|endoftext|>` 标记。这些 `<|endoftext|>` 标记作为标记，标志着特定段落的开始或结束，从而允许LLM更有效地处理和理解这些文本。

#### 修改词汇表以包括特殊标记
让我们通过在之前创建的所有唯一单词列表中添加这些特殊标记 `<|endoftext|>` 和 `<|unk|>` 来修改词汇表：

```python
all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token: integer for integer, token in enumerate(all_tokens)}

print(len(vocab.items()))
```

根据上述代码的输出，新词汇表的大小为1161（前一节中的词汇表大小为1159）。为了进行额外的快速检查，让我们打印更新后的词汇表的最后5个条目：

```python
for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)
```

上述代码打印了以下内容：
```
('younger', 1156)
('your', 1157)
('yourself', 1158)
('<|endoftext|>', 1159)
('<|unk|>', 1160)
```

基于上述代码输出，我们可以确认这两个新的特殊标记已经成功地包含在词汇表中。接下来，我们根据代码清单2.3相应地调整标记器，如代码清单2.4所示：

#### 代码清单2.4：处理未知单词的简单文本标记器

```python
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        preprocessed = [item if item in self.str_to_int else "<|unk|>" for item in preprocessed]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
```

相比于我们在上一节代码清单2.3中实现的SimpleTokenizerV1，新版的SimpleTokenizerV2在处理未知单词时用 `<|unk|>` 标记替换。

#### 实践中使用新的标记器
为了测试新的标记器，我们将使用由两句独立且不相关的句子连接而成的一个简单文本样本：

```python
text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " <|endoftext|> ".join((text1, text2))
print(text)
```

输出如下：
```
'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'
```

接下来，让我们使用之前创建的词汇表对示例文本进行标记化：

```python
tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))
```

这打印了以下标记ID：
```
[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]
```

上述列表中包含的标记ID 1159表示 `<|endoftext|>` 分隔符标记，以及两个 1160 标记，用于表示未知单词。

让我们进行快速检查，解码这些标记ID：
```python
print(tokenizer.decode(tokenizer.encode(text)))
```

输出如下：
```
'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'
```

通过将解码后的文本与原始输入文本进行比较，我们可以确定训练数据集（Edith Wharton的短篇小说《The Verdict》）中没有包含单词 "Hello" 和 "palace"。

#### 讨论其他特殊标记
除了我们已经讨论的 `<|unk|>` 和 `<|endoftext|>` 标记，根据具体需求和应用场景，还可以考虑其他特殊标记，例如：
- `[BOS]`（序列开始）：这个标记用于标识文本的开始位置。
- `[EOS]`（序列结束）：这个标记用于标识文本的结束位置，特别适用于连接多个不相关的文本段落。
- `[PAD]`（填充）：在使用大于一个的批处理大小训练LLM时，为确保所有文本长度相同，可以使用 `[PAD]` 标记将较短的文本扩展到最长文本的长度。

需要注意的是，GPT模型使用的标记器只使用一个 `<|endoftext|>` 标记以简化处理。这个标记不仅用于分隔，还用于填充。
